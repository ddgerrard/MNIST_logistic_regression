{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@the1ju/simple-logistic-regression-using-keras-249e0cc9a970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model of a logistic classifier\n",
    "import os\n",
    "import gzip\n",
    "import six.moves.cPickle as pickle\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_logistic_model(input_dim, output_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(output_dim, input_dim=input_dim, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                7850      \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "nb_classes = 10\n",
    "nb_epoch = 20\n",
    "input_dim = 784\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, input_dim)\n",
    "X_test = X_test.reshape(10000, input_dim)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = build_logistic_model(input_dim, nb_classes)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dgerrard\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 1s 16us/step - loss: 1.3176 - acc: 0.6759 - val_loss: 0.8233 - val_acc: 0.8274\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.7205 - acc: 0.8401 - val_loss: 0.6097 - val_acc: 0.8621\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.5875 - acc: 0.8600 - val_loss: 0.5262 - val_acc: 0.8740\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.5247 - acc: 0.8702 - val_loss: 0.4796 - val_acc: 0.8815\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.4867 - acc: 0.8767 - val_loss: 0.4496 - val_acc: 0.8868\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.4608 - acc: 0.8812 - val_loss: 0.4281 - val_acc: 0.8911\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.4416 - acc: 0.8844 - val_loss: 0.4118 - val_acc: 0.8931\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.4267 - acc: 0.8875 - val_loss: 0.3991 - val_acc: 0.8969\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.4147 - acc: 0.8895 - val_loss: 0.3889 - val_acc: 0.8982\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.4048 - acc: 0.8916 - val_loss: 0.3800 - val_acc: 0.8996\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 1s 14us/step - loss: 0.3964 - acc: 0.8930 - val_loss: 0.3727 - val_acc: 0.9006\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.3891 - acc: 0.8945 - val_loss: 0.3663 - val_acc: 0.9027\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.3828 - acc: 0.8963 - val_loss: 0.3608 - val_acc: 0.9035\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.3772 - acc: 0.8975 - val_loss: 0.3559 - val_acc: 0.9051\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 1s 10us/step - loss: 0.3722 - acc: 0.8985 - val_loss: 0.3513 - val_acc: 0.9055\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.3677 - acc: 0.8998 - val_loss: 0.3475 - val_acc: 0.9069\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.3636 - acc: 0.9005 - val_loss: 0.3440 - val_acc: 0.9076\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 1s 11us/step - loss: 0.3599 - acc: 0.9017 - val_loss: 0.3411 - val_acc: 0.9076\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 1s 13us/step - loss: 0.3565 - acc: 0.9020 - val_loss: 0.3377 - val_acc: 0.9088\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 1s 12us/step - loss: 0.3534 - acc: 0.9029 - val_loss: 0.3352 - val_acc: 0.9092\n",
      "Test score: 0.3352055793404579\n",
      "Test accuracy: 0.9092\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    verbose=1, validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "# save model as json and yaml\n",
    "json_string = model.to_json()\n",
    "open('mnist_Logistic_model.json', 'w').write(json_string)\n",
    "yaml_string = model.to_yaml()\n",
    "open('mnist_Logistic_model.yaml', 'w').write(yaml_string)\n",
    "\n",
    "# save the weights in h5 format\n",
    "model.save_weights('mnist_Logistic_wts.h5')\n",
    "\n",
    "# to read a saved model and weights\n",
    "# model = model_from_json(open('my_model_architecture.json').read())\n",
    "# model = model_from_yaml(open('my_model_architecture.yaml').read())\n",
    "# model.load_weights('my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
